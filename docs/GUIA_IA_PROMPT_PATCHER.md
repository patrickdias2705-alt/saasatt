# ü§ñ Guia Completo: Sistema de Patch Inteligente com IA

## üìã Vis√£o Geral

Este sistema usa IA generativa (Claude/GPT) para fazer atualiza√ß√µes **cir√∫rgicas** em prompts grandes, substituindo apenas a se√ß√£o espec√≠fica de um bloco sem alterar o resto do prompt.

## üéØ Por que usar IA ao inv√©s de SQL?

### Problemas com SQL/Regex:
- ‚ùå Fr√°gil a varia√ß√µes de formato
- ‚ùå Quebra com pequenas mudan√ßas na estrutura
- ‚ùå Dif√≠cil de manter quando o formato muda
- ‚ùå Requer m√∫ltiplas fun√ß√µes SQL complexas

### Vantagens da IA:
- ‚úÖ **Tolerante a varia√ß√µes** - entende contexto, n√£o apenas padr√µes
- ‚úÖ **Mais inteligente** - identifica se√ß√µes mesmo com formata√ß√£o diferente
- ‚úÖ **Mais f√°cil de manter** - mudan√ßas no formato n√£o quebram o sistema
- ‚úÖ **Mais precisa** - entende a estrutura sem depender de regex exato

## üèóÔ∏è Arquitetura

```
flow_blocks (UPDATE) 
    ‚Üì
Trigger PostgreSQL (opcional - pode chamar API)
    ‚Üì
API FastAPI (/api/flows/ai-patch-prompt)
    ‚Üì
Servi√ßo Python (ai_prompt_patcher.py)
    ‚Üì
Chama IA (Claude/GPT) com prompt estruturado
    ‚Üì
IA retorna prompt atualizado
    ‚Üì
UPDATE assistentes.prompt_voz
```

## üì¶ Instala√ß√£o

### 1. Instalar depend√™ncias Python

```bash
cd saas_server
pip install anthropic openai
```

### 2. Configurar vari√°veis de ambiente

Adicione ao seu `.env` ou vari√°veis de ambiente:

```bash
# Para usar Claude (Anthropic)
ANTHROPIC_API_KEY=sk-ant-...

# OU para usar OpenAI (GPT)
OPENAI_API_KEY=sk-...

# Opcional: escolher modelo espec√≠fico
ANTHROPIC_MODEL=claude-3-haiku-20240307  # Mais barato e r√°pido
# ANTHROPIC_MODEL=claude-3-opus-20240229  # Mais inteligente, mais caro

OPENAI_MODEL=gpt-4o-mini  # Mais barato
# OPENAI_MODEL=gpt-4o  # Mais inteligente
```

### 3. Obter API Keys

#### Anthropic (Claude):
1. Acesse: https://console.anthropic.com/
2. Crie uma conta ou fa√ßa login
3. V√° em "API Keys"
4. Crie uma nova chave
5. Copie e cole no `.env`

#### OpenAI (GPT):
1. Acesse: https://platform.openai.com/api-keys
2. Crie uma conta ou fa√ßa login
3. Crie uma nova chave
4. Copie e cole no `.env`

## üöÄ Como Usar

### Op√ß√£o 1: Via API REST (Recomendado)

```bash
POST http://localhost:8080/api/flows/ai-patch-prompt
Content-Type: application/json

{
  "assistente_id": "e7dfde93-35d2-44ee-8c4b-589fd408d00b",
  "block_key": "ENC001",
  "block_type": "encerrar",
  "new_content": "Desculpe pelo engano. At√© logooooo!",
  "provider": "anthropic"
}
```

**Resposta:**
```json
{
  "success": true,
  "updated_prompt": "...",
  "prompt_length_before": 5000,
  "prompt_length_after": 5010,
  "error": null
}
```

### Op√ß√£o 2: Integrar no Trigger PostgreSQL

Voc√™ pode modificar o trigger SQL para chamar a API quando necess√°rio:

```sql
-- Exemplo: Chamar API quando o trigger SQL falhar
CREATE OR REPLACE FUNCTION sync_prompt_voz_with_ai_fallback()
RETURNS TRIGGER AS $$
DECLARE
    v_updated_prompt TEXT;
    v_api_response JSONB;
BEGIN
    -- Tentar primeiro com SQL (m√©todo atual)
    v_updated_prompt := patch_block_section_in_prompt(...);
    
    -- Se n√£o funcionou, chamar API com IA
    IF v_updated_prompt = OLD.prompt_voz THEN
        -- Chamar API via pg_net (extens√£o Supabase)
        SELECT content INTO v_api_response
        FROM http((
            'POST',
            'http://localhost:8080/api/flows/ai-patch-prompt',
            ARRAY[
                http_header('Content-Type', 'application/json')
            ],
            'application/json',
            json_build_object(
                'assistente_id', NEW.assistente_id,
                'block_key', NEW.block_key,
                'block_type', NEW.block_type,
                'new_content', NEW.content,
                'provider', 'anthropic'
            )::text
        )::http_request);
        
        -- Extrair prompt atualizado da resposta
        v_updated_prompt := v_api_response->>'updated_prompt';
    END IF;
    
    -- Atualizar prompt_voz
    UPDATE assistentes SET prompt_voz = v_updated_prompt WHERE id = NEW.assistente_id;
    
    RETURN NEW;
END;
$$ LANGUAGE plpgsql;
```

### Op√ß√£o 3: Usar diretamente no Python

```python
from saas_tools.services.ai_prompt_patcher import patch_prompt_with_ai

# Fazer patch
updated_prompt = patch_prompt_with_ai(
    original_prompt=assistant["prompt_voz"],
    block_key="ENC001",
    block_type="encerrar",
    new_content="Desculpe pelo engano. At√© logooooo!",
    provider="anthropic"
)

# Atualizar no banco
supabase.table("assistentes").update({
    "prompt_voz": updated_prompt
}).eq("id", assistente_id).execute()
```

## üß™ Testando

### 1. Teste manual via curl

```bash
curl -X POST http://localhost:8080/api/flows/ai-patch-prompt \
  -H "Content-Type: application/json" \
  -d '{
    "assistente_id": "e7dfde93-35d2-44ee-8c4b-589fd408d00b",
    "block_key": "ENC001",
    "block_type": "encerrar",
    "new_content": "Desculpe pelo engano. At√© logooooo!",
    "provider": "anthropic"
  }'
```

### 2. Teste via Python

```python
import requests

response = requests.post(
    "http://localhost:8080/api/flows/ai-patch-prompt",
    json={
        "assistente_id": "e7dfde93-35d2-44ee-8c4b-589fd408d00b",
        "block_key": "ENC001",
        "block_type": "encerrar",
        "new_content": "Desculpe pelo engano. At√© logooooo!",
        "provider": "anthropic"
    }
)

print(response.json())
```

## üí∞ Custos

### Claude (Anthropic)
- **Haiku**: ~$0.25 por 1M tokens de entrada, $1.25 por 1M tokens de sa√≠da
- **Opus**: ~$15 por 1M tokens de entrada, $75 por 1M tokens de sa√≠da

**Estimativa por patch:**
- Prompt m√©dio: ~5.000 tokens
- Resposta: ~5.000 tokens
- **Custo com Haiku**: ~$0.0075 por patch (menos de 1 centavo)
- **Custo com Opus**: ~$0.45 por patch

### OpenAI (GPT)
- **GPT-4o-mini**: ~$0.15 por 1M tokens de entrada, $0.60 por 1M tokens de sa√≠da
- **GPT-4o**: ~$5 por 1M tokens de entrada, $15 por 1M tokens de sa√≠da

**Estimativa por patch:**
- **Custo com GPT-4o-mini**: ~$0.00375 por patch (menos de meio centavo)
- **Custo com GPT-4o**: ~$0.10 por patch

## ‚öôÔ∏è Configura√ß√£o Avan√ßada

### Escolher modelo baseado no tamanho do prompt

```python
def get_model_for_prompt(prompt_length: int) -> str:
    """Escolhe modelo baseado no tamanho do prompt"""
    if prompt_length < 10000:
        return "claude-3-haiku-20240307"  # Mais barato para prompts pequenos
    else:
        return "claude-3-sonnet-20240229"  # Mais inteligente para prompts grandes
```

### Cache de resultados

```python
from functools import lru_cache

@lru_cache(maxsize=100)
def cached_patch(original_prompt_hash: str, block_key: str, new_content: str) -> str:
    """Cache de patches para evitar chamadas duplicadas"""
    return patch_prompt_with_ai(...)
```

### Retry com fallback

```python
def patch_with_retry(original_prompt: str, block_key: str, block_type: str, 
                    new_content: str, max_retries: int = 3) -> str:
    """Tenta fazer patch com retry e fallback"""
    for attempt in range(max_retries):
        try:
            return patch_prompt_with_ai(
                original_prompt, block_key, block_type, new_content,
                provider="anthropic"
            )
        except Exception as e:
            if attempt == max_retries - 1:
                # √öltima tentativa: usar OpenAI como fallback
                return patch_prompt_with_ai(
                    original_prompt, block_key, block_type, new_content,
                    provider="openai"
                )
            time.sleep(2 ** attempt)  # Exponential backoff
```

## üîç Debugging

### Ver logs

```python
import logging
logging.basicConfig(level=logging.INFO)
```

Os logs mostrar√£o:
- Quando o patch √© iniciado
- Qual bloco est√° sendo atualizado
- Se o patch foi bem-sucedido
- Tamanhos antes/depois

### Verificar se a IA encontrou a se√ß√£o

A IA retorna o prompt completo. Compare com o original:

```python
original = assistant["prompt_voz"]
updated = response["updated_prompt"]

# Verificar se mudou
if original != updated:
    print("‚úÖ Patch aplicado com sucesso")
    # Ver diferen√ßa
    import difflib
    diff = difflib.unified_diff(original.splitlines(), updated.splitlines())
    for line in diff:
        print(line)
else:
    print("‚ö†Ô∏è Prompt n√£o mudou - IA pode n√£o ter encontrado a se√ß√£o")
```

## üö® Troubleshooting

### Erro: "ANTHROPIC_API_KEY n√£o configurada"
- Verifique se a vari√°vel de ambiente est√° definida
- Reinicie o servidor ap√≥s adicionar a vari√°vel

### Erro: "Assistente n√£o encontrado"
- Verifique se o `assistente_id` est√° correto
- Verifique se o assistente tem `prompt_voz` preenchido

### IA n√£o est√° atualizando o prompt
- Verifique os logs para ver o que a IA retornou
- Pode ser que o formato do bloco no prompt original seja muito diferente
- Tente usar um modelo mais inteligente (Opus ao inv√©s de Haiku)

### Custo muito alto
- Use modelos mais baratos (Haiku, GPT-4o-mini)
- Implemente cache para evitar chamadas duplicadas
- Considere usar SQL para casos simples e IA apenas quando necess√°rio

## üìö Refer√™ncias

- [Anthropic API Docs](https://docs.anthropic.com/)
- [OpenAI API Docs](https://platform.openai.com/docs)
- [Supabase Edge Functions](https://supabase.com/docs/guides/functions)
- [FastAPI Docs](https://fastapi.tiangolo.com/)

## ‚úÖ Checklist de Implementa√ß√£o

- [ ] Instalar depend√™ncias (`anthropic` ou `openai`)
- [ ] Configurar API keys no `.env`
- [ ] Testar endpoint `/api/flows/ai-patch-prompt`
- [ ] Verificar logs de sucesso/erro
- [ ] (Opcional) Integrar com trigger SQL
- [ ] (Opcional) Implementar cache
- [ ] (Opcional) Configurar retry/fallback
